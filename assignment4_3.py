# -*- coding: utf-8 -*-
"""Assignment4.3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t_geGYVQe_295Nfjz7ubskJcBCFf2qc_
"""

import numpy as np 
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import cifar10,mnist

(x_train,y_train),(x_test,y_test) = cifar10.load_data()

Ntr = x_train.shape[0]
Nte = x_test.shape[0]
Din = 3072

x_train = x_train[range(Ntr),:]
x_test = x_test[range(Nte),:]
y_train = y_train[range(Ntr)]
y_test=y_test[range(Nte)]
k=len(np.unique(y_train))

y_train = tf.keras.utils.to_categorical(y_train,num_classes=k)
y_test = tf.keras.utils.to_categorical(y_test,num_classes=k)
print("x_train - > ",x_train.shape) #training images
print("y_train - > ",y_train.shape) #labels
print("x_test - > ",x_test.shape) #training images
print("y_test - > ",y_test.shape)

x_train = np.reshape(x_train,(Ntr,Din))
x_test = np.reshape(x_test,(Nte,Din))
x_train = x_train.astype('float32')
x_test = x_test.astype("float32")

mean_image = np.mean(x_train,axis = 0)
x_train = x_train - mean_image
x_test = x_test - mean_image

H=200     #number of hidden nodes
std =1e-6 #standard deviation
w1 = std*np.random.randn(Din,H) 
w2 = std*np.random.randn(H,k)
b1 =np.zeros(H)
b2 = np.zeros(k)

batch_size = 50000 #total batch size
iterations = 100   #number of epochs
lr = 1.0e-2        #learning rate
lr_decay = 0.999   
reg = 5e-6         #lamda 
loss_history =[]
train_acc_history = []
val_acc_history = []
batch_indices = np.random.choice(Ntr,batch_size) #shuffling batch indices
sub_batch_size = 500                             #sample batch size
num_groups = batch_size/sub_batch_size           #number of samples

for i in range(iterations): #looping each epoch
  for j in range(int(num_groups)):#looping samples
    sub_batch_indices = batch_indices[j*500:(j+1)*500] #selecting samples
    x=x_train[sub_batch_indices] #sample images
    y=y_train[sub_batch_indices] #corresponding labels
    h=1.0/(1.0+np.exp(-(x.dot(w1)+b1))) #sigmoid function
    y_pred =h.dot(w2)+b2                #Prediction
    loss= 1./sub_batch_size*np.square(y_pred-y).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1)) #loss funtion
    loss_history.append(loss)
    if j%10 == 0:
      print('number of groups %d / number of iterations :%d:loss %f'%(j,i,loss))
  
    dy_pred = 1./sub_batch_size*2.0*(y_pred-y) #patial derivatives
    dw2 = h.T.dot(dy_pred) + reg*w2
    db2 = dy_pred.sum(axis=0)
    dh = dy_pred.dot(w2.T)
    dw1 = x.T.dot(dh*h*(1-h))+reg*w1
    db1 =(dh*h*(1-h)).sum(axis =0)
    w1 -= lr*dw1 #updating
    w2 -= lr*dw2
    b1 -= lr*db1
    b2 -= lr*db2
    lr *= lr_decay

#Calculating test loss
test_loss = 0
batch_indices = np.arange(Nte)
x=x_test[batch_indices]
y=y_test[batch_indices]
h=1.0/(1.0+np.exp(-(x.dot(w1)+b1)))
y_pred =h.dot(w2)+b2
test_loss= 1./10000*np.square(y_pred-y).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1)) #batch_size= 10000
print("test loss ->",test_loss)


#Plotting W images
fig,ax =plt.subplots(1,10)
fig.set_size_inches(32,10)
for i in range(10):
  img = w1[:,i].reshape(32,32,3)
  nor_img =255*(img-np.min(w1))/(np.max(w1)-np.min(w1))
  ax[i].imshow(nor_img.astype('uint8'))
plt.show()

#plotting train loss
plt.plot(loss_history)
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.title("Loss history")
plt.show()

# Calculating train accuracy
x_t = x_train
print("x_train->",x_t.shape)
h=1.0/(1.0+np.exp(-(x_t.dot(w1)+b1)))
y_pred = h.dot(w2)+b2

train_acc = 1.0 - 1/(Ntr*9)*(np.abs(np.argmax(y_train,axis=1)-np.argmax(y_pred,axis=1))).sum()
print("train_acc =",train_acc)

#calculating test accuracy
x_t=x_test
print("x_test->",x_t.shape)
h=1.0/(1.0+np.exp(-(x_t.dot(w1)+b1)))
y_pred = h.dot(w2)+b2

test_acc = 1.0-1/(Nte*9)*(np.abs(np.argmax(y_test,axis=1)-np.argmax(y_pred,axis=1))).sum()
print("test_acc = ",test_acc)